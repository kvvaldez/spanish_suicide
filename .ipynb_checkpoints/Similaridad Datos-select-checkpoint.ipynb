{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usa frecuencia de palabras\n",
    "def create_tf_idf(list_text):\n",
    "    vectorizer = TfidfVectorizer(use_idf=False, norm=None, ngram_range=(1, 1)) # use_idf=true ponderacion de frecuancia de palabras en documentos\n",
    "    tfidf = vectorizer.fit(list(list_text))\n",
    "    vectores_tfidf=vectorizer.transform(list_text)\n",
    "    dump(tfidf,'tfidf_model2joblib')\n",
    "    print('Fue creado modelo TF-IDF')\n",
    "    #vectores_tfidf=list(matrix)\n",
    "    #print(len(vectores_tfidf))\n",
    "    return vectores_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna un vector TF-IDF \n",
    "def get_vector_tfidf(list_text):\n",
    "    tfidf=load('tfidf_model2.joblib')# Ojo cuidado, solo cargar solo una vez\n",
    "    words_ntf=[x for x in list_text.split() if x not in tfidf.get_feature_names()] # Palabras que no estan en el vocabulario del modelo\n",
    "    return tfidf.transform([list_text]).toarray()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return sum([i*j for i,j in zip(a, b)])/(math.sqrt(sum([i*i for i in a]))* math.sqrt(sum([i*i for i in b])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leyendo los datos CSV\n",
    "datos_notacion = pd.read_csv('natacion_suicidio.csv')\n",
    "datos_general = pd.read_csv('suicidio_todo.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datos_notacion.size)\n",
    "print(datos_general.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seleccionamos solo los datos que tienen tendencia suicida\n",
    "selec_dat_1=datos_notacion.loc[datos_notacion['suicidio'] == 1]\n",
    "selec_dat_1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_general.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_k_elements(group, k=200): \n",
    "    if len(group) < k:\n",
    "        return group\n",
    "    return group.sample(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos y guardamos en un diccionario, cada elemento del diccionario es una lista\n",
    "dict_general = datos_general.groupby('frase').apply(sampling_k_elements).reset_index(drop=True)\n",
    "dict_general=dict_general.groupby('frase')['tweet_clean'].apply(list).to_dict()\n",
    "\n",
    "# Extraemos todo los tweets para crear el vocabulario para TF-IDF\n",
    "text_todo=list(datos_general.tweet_clean.values.astype('U'))\n",
    "vectores_tfidf=create_tf_idf(text_todo) #vocubulario total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dict_general.keys():\n",
    "    print(x,len(dict_general[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(selec_dat_1.frase)[95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similares=defaultdict(list) #inicializamos un diccionario de listas para guardar la data necesaria\n",
    "for i,j in zip(list(selec_dat_1.frase)[95:],list(selec_dat_1.tweet_clean)[95:]):\n",
    "    vec_selec=get_vector_tfidf(j)\n",
    "    print(i)\n",
    "    if i in dict_general:\n",
    "        for x in dict_general[i]:\n",
    "            vec_todo_j=get_vector_tfidf(x)\n",
    "            simi=cosine_similarity(vec_selec,vec_todo_j)[0][0]\n",
    "            if simi>0.6 and simi<0.95 and x in not similares['tweet_similar'] : # guardamos todos los tweets con similaridad alta, pero menor 0.95 que podria ser la misma frase\n",
    "                similares['frase'].append(i)\n",
    "                similares['tweet_anotado'].append(j)\n",
    "                similares['tweet_similar'].append(x)\n",
    "                similares['similaridad'].append(simi)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertimos el diccionario de listas a un Dataframe e lo guardamos como CSV\n",
    "findal_dt=pd.DataFrame.from_dict(similares,orient='index').transpose()\n",
    "findal_dt.to_csv(r'datos_similares_notados_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar data para re-anotacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_notar = pd.read_csv('datos_similares_notados1.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_notar.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=list(datos_notar.tweet_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=list(set(ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx = pd.DataFrame({'tweet_clean':ts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx.to_csv(r'mas.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
